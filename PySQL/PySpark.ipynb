{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Managing Imports\n",
    "from time import time\n",
    "import numpy as np\n",
    "import operator\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark import SparkContext\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If the notebook is run locally, then sc (SparkContext) would be pre-configured. If running using binder, we need to create SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This notebook comes with a pre-configured sparkContext called sc\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    sc = SparkContext(master='spark://master:7077')\n",
    "    with open(\"data/sequence.txt\") as f:\n",
    "         sequence = [x.strip('\\n') for x in f.readlines()]\n",
    "    file_rdd = sc.parallelize(sequence)\n",
    "    with open(\"data/people.json\") as f:\n",
    "        json_data = [x.strip('\\n') for x in f.readlines()]\n",
    "    json_rdd = sc.parallelize(json_data)\n",
    "else:\n",
    "    file_rdd = sc.textFile(\"data/sequence.txt\")\n",
    "    json_rdd = sc.textFile(\"data/people.json\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RDDs\n",
    "dummy_list = range(1000)\n",
    "#this is a list\n",
    "print type(dummy_list)\n",
    "rddlist = sc.parallelize(dummy_list)\n",
    "#this is a RDD\n",
    "print type(rddlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#More RDDs\n",
    "print json_rdd.collect()\n",
    "print type(json_rdd.collect())\n",
    "#Spark supports text files, SequenceFiles, and any other Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "filtered_rdd = file_rdd.filter(lambda x: len(x) < 2)\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "filtered_data = file_rdd.filter(lambda x: len(x) < 2).collect()\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)\n",
    "print filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "mapped_rdd = file_rdd.map(lambda x: 2*x)\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "mapped_data = file_rdd.map(lambda x: 2*x).collect()\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(filtered_rdd)\n",
    "print type(filtered_data)\n",
    "print type(mapped_rdd)\n",
    "print type(mapped_data)\n",
    "print \"Conclusion: RDDs are lazy. They do nothing unless there is an action is called.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "print len(file_rdd.map(lambda x: 2*x).filter(lambda x: len(x)>1).take(10))\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "print len(file_rdd.map(lambda x: 2*x).filter(lambda x: len(x)>1).take(100000))\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "print len(file_rdd.map(lambda x: 2*x).filter(lambda x: len(x)>1).collect())\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Lazy is better.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "print \"We want to count the number of 1, 2, ... digit numbers.\"\n",
    "# file_path = \"data/sequence.txt\"\n",
    "# file_rdd = sc.textFile(file_path) \n",
    "mapped_rdd = file_rdd.map(lambda a: (len(a), 1))\n",
    "count_rdd = mapped_rdd.reduceByKey(lambda a, b: a+b).sortByKey()\n",
    "print count_rdd.collect()\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "print \"We want to count the number of 1, 2, ... digit numbers.\"\n",
    "mapped_data = np.asarray(map(lambda a: (len(a), 1), file_rdd.collect()))\n",
    "# count_map = mapped_rdd.reduceByKey(lambda a, b: a+b).sortByKey().collect()\n",
    "# print count_map\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Lets add all the numbers. We have two ways of doing that.\"\n",
    "print \"Approach 1: update a counter variable\"\n",
    "counter = 0\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter+=x\n",
    "mapped_rdd = file_rdd.map(lambda a: int(a))\n",
    "mapped_rdd.foreach(increment_counter)\n",
    "print \"Sum using first approach: \", counter\n",
    "print \"Approach 2: use reduce operation\"\n",
    "print \"Sum using second appraoch: \", mapped_rdd.reduce(operator.add)\n",
    "print \"Which one is correct?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "mapped_rdd.foreach(lambda x: accum.add(x))\n",
    "print(\"Actual sum: \", accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"So far we talked about big data. What about structured big data?\"\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataframes\n",
    "df = sqlContext.read.json(\"data/people.json\")\n",
    "df.show()\n",
    "print \"A DataFrame is the structured version of RDD. This is the familiar relational view of the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "print \"DF can infer schema on its own. We can always override the inferred schema.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_df = file_rdd.map(lambda a: Row(a)).toDF()\n",
    "rdd_df.show()\n",
    "rdd_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rdd_df = file_rdd.map(lambda a: Row(int(a))).toDF()\n",
    "# rdd_df.show()\n",
    "# rdd_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"DF can be queried in two ways: API and sql queries\"\n",
    "print \"First method: API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select(df['first_name'], df['gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy(df[\"gender\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy(df[\"email\"]).agg(func.count(df[\"email\"]).alias(\"count\")).orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Second method: sql\"\n",
    "df.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT * FROM people WHERE gender = 'Female'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"We can define our own functions as well.\"\n",
    "domain = func.udf(lambda s: s.split('@')[1], StringType())\n",
    "df.select(domain(df.email).alias('domain'))\\\n",
    ".groupBy('domain').agg(func.count('domain').alias(\"count\")).orderBy(\"count\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
